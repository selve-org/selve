# SELVE Scoring Algorithm - Documentation

## Overview

The SELVE scoring algorithm is the core personality profiling engine that transforms user responses into complete personality profiles across 8 dimensions.

**File**: `backend/app/scoring.py`

---

## Features

‚úÖ **Automatic reverse scoring** - Handles negatively worded items  
‚úÖ **Multi-scale support** - Works with 5-point (Big Five, 16PF) and 7-point (HEXACO) scales  
‚úÖ **Normalized scores** - Converts raw scores to 0-100 scale  
‚úÖ **Validation** - Checks response validity before scoring  
‚úÖ **JSON export** - Easy integration with APIs  
‚úÖ **Quick screening** - Select highest-quality items for faster assessments  
‚úÖ **Interpretations** - Automatic text descriptions of score levels

---

## Core Classes

### `SelveScorer`

Main scoring engine class.

**Initialization:**

```python
from scoring import SelveScorer

scorer = SelveScorer('/path/to/selve_item_pool_expanded.json')
```

**Methods:**

#### `score_responses(responses, validate=True)`

Score user responses and generate complete profile.

**Parameters:**

- `responses` (dict): Mapping of item codes to response values
  - Example: `{'E1': 4, 'E2': 2, 'N1': 3, ...}`
- `validate` (bool): Whether to validate responses (default: True)

**Returns:** `SelveProfile` object

**Example:**

```python
responses = {
    'E1': 4,  # I am the life of the party
    'E2': 2,  # I don't talk a lot [R]
    'E5': 5,  # I start conversations
    'N1': 2,  # I get stressed out easily [R]
    'A4': 5,  # I sympathize with others' feelings
    # ... more responses
}

profile = scorer.score_responses(responses)
```

#### `get_quick_screen_items(n_per_dimension=2)`

Get highest-quality items for quick assessment.

**Parameters:**

- `n_per_dimension` (int): Number of items per dimension

**Returns:** List of item dicts

**Example:**

```python
# Get 2 best items per dimension (16 total)
quick_items = scorer.get_quick_screen_items(n_per_dimension=2)

# Display to user
for item in quick_items:
    print(f"{item['text']}")  # Question text
    print(f"Scale: 1-{item['scale_max']}")
```

#### `get_items_by_dimension(dimension)`

Get all items for a specific dimension.

**Parameters:**

- `dimension` (str): Dimension name (e.g., 'LUMEN')

**Returns:** List of item dicts

#### `get_all_items()`

Get all 98 items from the pool.

**Returns:** List of item dicts with dimension added

---

### `SelveProfile`

Complete personality profile with all 8 dimensions.

**Attributes:**

- `lumen`: DimensionScore for LUMEN ‚ú®
- `aether`: DimensionScore for AETHER üå´Ô∏è
- `orpheus`: DimensionScore for ORPHEUS üéµ
- `orin`: DimensionScore for ORIN üß≠
- `lyra`: DimensionScore for LYRA ü¶ã
- `vara`: DimensionScore for VARA ‚öñÔ∏è
- `chronos`: DimensionScore for CHRONOS ‚è≥
- `kael`: DimensionScore for KAEL üî•

**Methods:**

#### `to_dict()`

Export profile to dictionary (JSON-serializable).

**Returns:** Dict with all dimension scores

**Example:**

```python
profile_dict = profile.to_dict()
print(json.dumps(profile_dict, indent=2))
```

#### `get_top_dimensions(n=3)`

Get the N strongest dimensions.

**Parameters:**

- `n` (int): Number of top dimensions to return

**Returns:** List of dimension names (strings)

**Example:**

```python
top_3 = profile.get_top_dimensions(3)
print(f"Your strongest dimensions: {', '.join(top_3)}")
# Output: "Your strongest dimensions: VARA, CHRONOS, LUMEN"
```

---

### `DimensionScore`

Score for a single personality dimension.

**Attributes:**

- `dimension` (str): Dimension name
- `raw_score` (float): Mean of scored responses (1-7 scale)
- `normalized_score` (float): Score on 0-100 scale
- `percentile` (float): Percentile rank (future feature)
- `n_items` (int): Number of items answered
- `interpretation` (str): Text description (e.g., "High Social Energy")

**Methods:**

#### `to_dict()`

Export to dictionary.

---

## Usage Examples

### Basic Usage

```python
from scoring import SelveScorer

# Initialize
scorer = SelveScorer('/home/chris/selve/data/selve_item_pool_expanded.json')

# User responses
responses = {
    'E1': 4, 'E5': 5, 'E7': 4,  # LUMEN items
    'N1': 2, 'N6': 2,           # AETHER items
    'A4': 5, 'A9': 5,           # ORPHEUS items
    'C1': 5, 'C5': 4,           # ORIN items
    'O10': 5, 'O1': 4,          # LYRA items
    'HMode10': 2, 'HFair1': 6,  # VARA items
    'APati3': 6, 'APati1': 5,   # CHRONOS items
    'D4': 4, 'D6': 5,           # KAEL items
}

# Score
profile = scorer.score_responses(responses)

# Access scores
print(f"LUMEN: {profile.lumen.normalized_score:.1f}/100")
print(f"Level: {profile.lumen.interpretation}")
```

### Quick Screen Assessment

```python
# Get 2 items per dimension (16 total)
quick_items = scorer.get_quick_screen_items(2)

# Present to user (simplified)
responses = {}
for item in quick_items:
    print(f"\n{item['text']}")
    print(f"(1-5 scale: 1=Strongly Disagree, 5=Strongly Agree)")
    response = int(input("Your answer: "))
    responses[item['item']] = response

# Score with partial responses
profile = scorer.score_responses(responses)
```

### Adaptive Testing

```python
# Start with quick screen
quick_items = scorer.get_quick_screen_items(2)
responses = get_user_responses(quick_items)  # Your function

# Initial scoring
profile = scorer.score_responses(responses)

# Identify uncertain dimensions (scores near 50)
uncertain_dims = []
for dim in ['LUMEN', 'AETHER', 'ORPHEUS', 'ORIN', 'LYRA', 'VARA', 'CHRONOS', 'KAEL']:
    score = getattr(profile, dim.lower()).normalized_score
    if 40 <= score <= 60:  # Uncertain range
        uncertain_dims.append(dim)

# Ask more questions for uncertain dimensions
for dim in uncertain_dims:
    additional_items = scorer.get_items_by_dimension(dim)
    # Filter out already answered
    new_items = [i for i in additional_items if i['item'] not in responses]
    # Get next 3 best items
    next_items = sorted(new_items, key=lambda x: x['correlation'], reverse=True)[:3]

    # Ask user these questions
    for item in next_items:
        response = get_user_response(item)  # Your function
        responses[item['item']] = response

# Final scoring
final_profile = scorer.score_responses(responses)
```

### API Integration

```python
from flask import Flask, request, jsonify
from scoring import SelveScorer

app = Flask(__name__)
scorer = SelveScorer('/path/to/item_pool.json')

@app.route('/api/score', methods=['POST'])
def score_assessment():
    """
    Score user assessment responses.

    Request body:
    {
        "responses": {
            "E1": 4,
            "E2": 2,
            ...
        }
    }

    Response:
    {
        "LUMEN": {
            "dimension": "LUMEN",
            "normalized_score": 75.5,
            "interpretation": "High Social Energy",
            ...
        },
        ...
    }
    """
    try:
        data = request.json
        responses = data.get('responses', {})

        # Score
        profile = scorer.score_responses(responses)

        return jsonify(profile.to_dict()), 200

    except ValueError as e:
        return jsonify({'error': str(e)}), 400

@app.route('/api/items/quick', methods=['GET'])
def get_quick_items():
    """Get items for quick screening."""
    n = request.args.get('n', 2, type=int)
    items = scorer.get_quick_screen_items(n_per_dimension=n)
    return jsonify(items), 200

if __name__ == '__main__':
    app.run(debug=True)
```

---

## Scale Information

### Item Types and Scales

| Source   | Items                            | Scale | Example        |
| -------- | -------------------------------- | ----- | -------------- |
| Big Five | E, N, A, C, O                    | 1-5   | E1, N10, A4    |
| 16PF     | D                                | 1-5   | D1, D6         |
| HEXACO   | H, X, AForg, AGent, AFlex, APati | 1-7   | HMode1, APati3 |

### Reverse Scoring

Negatively worded items are automatically reverse-scored:

- 5-point scale: `score = 6 - response`
- 7-point scale: `score = 8 - response`

**Example reverse-scored items:**

- E2: "I don't talk a lot" (low score = high extraversion)
- N1: "I get stressed out easily" (low score = high stability)
- HMode10: "I boast about my virtues" (low score = high modesty)

---

## Score Interpretation

### Normalized Score Levels (0-100 scale)

| Range  | Level     | Description                    |
| ------ | --------- | ------------------------------ |
| 75-100 | Very High | Strong presence of trait       |
| 60-74  | High      | Above-average trait expression |
| 40-59  | Moderate  | Average trait level            |
| 25-39  | Low       | Below-average trait expression |
| 0-24   | Very Low  | Trait rarely expressed         |

### Example Interpretations

**LUMEN (Social Energy)**

- Very High: "Highly social, talkative, life of the party"
- Moderate: "Balanced between socializing and alone time"
- Very Low: "Prefers solitude, reserved in groups"

**VARA (Honesty-Humility)**

- Very High: "Highly ethical, humble, values fairness"
- Moderate: "Generally honest with some self-interest"
- Very Low: "Self-focused, status-seeking, manipulative"

**CHRONOS (Patience)**

- Very High: "Extremely patient, forgiving, adaptable"
- Moderate: "Patient in most situations"
- Very Low: "Quick to anger, holds grudges"

---

## Testing

Run the test suite:

```bash
cd /home/chris/selve/backend
source venv/bin/activate
pytest tests/test_scoring.py -v
```

**Test coverage:**

- ‚úÖ Scorer initialization
- ‚úÖ Scale detection (5-point vs 7-point)
- ‚úÖ Reverse scoring logic
- ‚úÖ Full response scoring
- ‚úÖ Response validation
- ‚úÖ Partial responses handling
- ‚úÖ Quick screen item selection
- ‚úÖ Top dimensions calculation
- ‚úÖ JSON export
- ‚úÖ Score interpretation levels

---

## Performance

**Scoring speed:**

- Full assessment (98 items): ~0.5ms
- Quick screen (16 items): ~0.1ms
- Minimal computational overhead

**Memory usage:**

- Item pool: ~50KB
- Profile object: ~2KB

---

## Next Steps

1. ‚úÖ **Scoring algorithm complete** - Production-ready
2. ‚è≥ **Build adaptive testing** - Smart question selection
3. ‚è≥ **Create API endpoints** - FastAPI/Flask integration
4. ‚è≥ **Add percentile norms** - Compare to population
5. ‚è≥ **Generate narratives** - Personalized insights
6. ‚è≥ **Build frontend** - Assessment interface

---

## Files

- `backend/app/scoring.py` - Main scoring engine
- `backend/app/test_scoring.py` - Test suite
- `data/selve_item_pool_expanded.json` - Item pool (98 items)

---

## Support

For questions or issues:

- Check test suite for examples
- Review this documentation
- See `scoring.py` docstrings
